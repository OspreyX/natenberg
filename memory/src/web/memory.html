<html>
	<head>
		<title>Memory Barriers and the JVM</title>
		<style>
			.code{
				background-color: #CCCCCC;
				padding: 10px
			}
		</style>
	</head>
	<body>
		<h1>Memory Barriers and the JVM</h1>
		<p>
			Memory barriers, or fences, are a set of processor instructions used to apply ordering limitations on memory operations.  		
			This article explains the impact memory barriers have on the determinism of multi-threaded programs.
			We'll look at how memory barriers relate to JVM concurrency constructs such as volatile, synchronized and atomic conditionals. 
			It is assumed the reader has a solid understanding of these concepts and the Java memory model.
			This is not an article about mutual exclusion, parallelism or atomicity per se.
			Memory barriers are used to achieve an equally important element of concurrent programming called visibility.
		</p>
		<h3>Why Are Memory Barriers Important?</h3>
		<p>
			The cost of memory latency has pushed processors to reorder and cache memory operations.
			A trip to main memory costs hundreds of clock cycles on commodity hardware.
			Processors use caching to decrease latency costs of memory operations by orders of magnitude.
			These caches introduce a substantial visibility delay because the value of each write operation does not immediately "write through" to main memory.
			The processor instead places this operation in a buffer and is obligated to "write back" the value later.
			Buffers are often misunderstood as queues; they are not.
			Pending memory operations are not necessarily performed in the order in which they are fed to the processor.
			This opens the door for many optimizations when data is immutable and/or confined to the scope of one thread.  
		</p>
		<p>
			Combining these optimizations with symmetric multi-processing and shared mutable state is a nightmare.
			A program can behave non-deterministically when memory operations on shared mutable state are re-ordered.  
			It is possible for a thread to write values that become visible to another thread in ways that are inconsistent with the order in which they were written.
			A well placed memory barrier prevents this problem by forcing the processor to serialize pending memory operations.
			The performance implications of memory barriers are beyond the scope of this article.
		</p>
		<h3>Memory Barriers As Protocols</h3>
		<p>
			Memory barriers are not directly exposed by the JVM; 
			instead they are inserted into the instruction sequence by the JVM in order to uphold the semantics of language level concurrency primitives.
			We'll look at the source code and assembly instructions of some simple Java programs to see how. 
			Let's begin a crash course in memory barriers with the Dekker's algorithm.
			This algorithm uses three volatile variables to coordinate access to a shared resource between two threads.
		</p>
		<p>
			Try not to focus on the finer details of this algorithm.
			Which parts are relevant? 
			Each thread attempts to enter the critical section on the first line of code by signaling an intent to do so.
			If a thread observes a conflict on line three (both threads have signaled intent) the conflict is resolved by turn taking.
			Only one thread can access the critical section at a given point in time.				
		</p>
		<table>
			<tr>
				<td>
					Code run by the first thread:
				</td>
				<td style="padding-left:40px">
					Code run by the second thread:
				</td>				
			</tr>
			<tr>
				<td>
<pre>
 1    intentFirst = true;
 2
 3    while (intentSecond)
 4    	if (turn != 0) {
 5    		intentFirst = false;
 6    		while (turn != 0) {}
 7    		intentFirst = true;
 8    	}
 9
10    criticalSection();
11
12    turn = 1;
13    intentFirst = false;
</pre>				
				</td>
				<td style="padding-left:40px">
<pre>
intentSecond = true;

while (intentFirst)
  if (turn != 1) {
    intentSecond = false;
    while (turn != 1) {}
    intentSecond = true;
  }

criticalSection();

turn = 0;
intentSecond = false;
</pre>				
				</td>
			</tr>
		</table>
		<p>
			Hardware optimizations can break this code without memory barriers, even if the compiler were to emit all memory operations as they appear to the programmer.
			Consider the two successive read operations on lines three and four.
			Each thread checks to see if the other has signaled an intent to enter the critical section <i>and then</i> checks to see who's turn it is.
			Consider the two successive write operations on lines 12 and 13.
			Each thread gives the other it's "turn" <i>and then</i> withdraws it's intent to enter the critical section.
			A reading thread should never expect to observe the other thread's write to the turn variable after the other thread's withdrawal of intent.
			This would be a disaster.
			Without the volatile modifier on these variables this indeed can happen.
			For example, without the volatile modifier the second thread could observe the first thread's write to intentFirst (last line) before the first thread's write to turn (second to last line). 
			The keyword volatile prevents this problem because it establishes a <i>happens before</i> relationship between the write to the turn variable and the write to the intentFirst variable.
			The compiler cannot re-order these write operations and if necessary it must forbid the processor from doing so with memory barriers.
			A peek under the hood shows how.
		</p>
		<table>
			<tr>
				<td valign="top">
					<p>
						The first of the two successive read operations on line three is captured in the assembly instructions below.
						This stream was captured on multi-processing Itanium 2 hardware running <span style="color:red">TODO: JDK Version</span>. 
					</p>
				</td>
				<td>
					<div style="border: 1px solid black; padding: 10px">
					The PrintAssembly HotSpot option is a diagnostic flag for the JVM that allows us to capture the generated assembly instructions of the JIT compiler.
					This requires the latest OpenJDK release or a new version of HotSpot, update 14 or above.
					A disassembler plugin is also required. 
					The Kenai project has binaries for Solaris, Linux and BSD.
					The hsdis plugin is an alternative that can be built from source for Windows.
					</div>					
				</td>
			</tr>
		</table>
<pre>
1  0x2000000001de819c:      adds r37=597,r36;;  ;...84112554
<b>2  0x2000000001de81a0:      ld1.acq r38=[r37];;  ;...0b30014a a010</b>
3  0x2000000001de81a6:      nop.m 0x0     ;...00000002 00c0
4  0x2000000001de81ac:      sxt1 r38=r38;;  ;...00513004
5  0x2000000001de81b0:      cmp4.eq p0,p6=0,r38  ;...1100004c 8639
6  0x2000000001de81b6:      nop.i 0x0     ;...00000002 0003
7  0x2000000001de81bc:      br.cond.dpnt.many 0x2000000001de8220;;
</pre>	
		<p>
			This short stream of instructions tells a long story.
			The first volatile read is on line two.  
			The Java memory model guarantees the JVM will deliver this read to the processor before the second read, in "program order" - 
			but this alone would not be enough because the processor is still free to perform these operations out of order.
			To uphold the consistency guarantees of the Java memory model the JVM annotates the first read operation with a variant of ld.acq, or "load acquire".
			By using ld.acq the compiler ensures the read operation on line two will complete before the subsequent read operation.
			Problem solved.
		</p>
		<p>
			Notice this affects reads, not writes.
			A memory barrier that enforces ordering limitations on reads <i>or</i> writes is said to be unidirectional.
			A memory barrier that enforces ordering limitations on reads <i>and</i> writes is said to be bidirectional, or, a full fence.  
			Using ld.acq is an example of a unidirectional memory barrier.
		</p>
		<p>
			Consistency is a two way street.
			How useful is it for a reading thread to insert a memory barrier between both reads if the other thread does not separate both writes with one as well?
			In order for threads to communicate they must <i>all</i> obey the protocol; just like nodes on a network, or people on a team.
			If one thread breaks formation than the efforts of all other threads are rendered useless.
			We should expect to see a memory barrier between in the assembly instructions for the last two lines of Dekker's algorithm, a write followed by a write.
		</p>
		<!--div class="code">
			$ java -XX:+UnlockDiagnosticVMOptions -XX:PrintAssemblyOptions=hsdis-print-bytes -XX:CompileCommand=print,WriterReader.write WriterReader
		</div-->	
<pre>
 1  0x2000000001de81c0:      adds r37=592,r36;;  ;...0b284149 0421
 2  0x2000000001de81c6:      st4.rel [r37]=r39  ;...00389560 2380
 3  0x2000000001de81cc:      adds r36=596,r36;;  ;...84112544
 4<b>  0x2000000001de81d0:      st1.rel [r36]=r0  ;...09000048 a011</b>
 5<b>  0x2000000001de81d6:      mf            ;...00000044 0000</b>
 6  0x2000000001de81dc:      nop.i 0x0;;   ;...00040000
 7  0x2000000001de81e0:      mov r12=r33   ;...00600042 0021
 8  0x2000000001de81e6:      mov.ret b0=r35,0x2000000001de81e0
 9  0x2000000001de81ec:      mov.i ar.pfs=r34  ;...00aa0220
10  0x2000000001de81f0:      mov r6=r32    ;...09300040 0021
</pre>
		<p>
			Here we can see the second write operation annotated with explicit memory barrier on line four.
			By using a variant of st.rel, or "store release", the compiler ensures the first write operation will be visible before the second write operation.
			This completes both sides of the protocol because the first write operation <i>happens before</i> the second write operation.
		</p>
		<p>
			The st.rel barrier is unidirectional - just like ld.acq.  On line five however the compiler emits a bidirectional memory barrier.
			The mf instruction, or "memory fence", is a full fence for the Itanium 2 instruction set.
			This seems redundant to the author.
		</p>
		<h3>Memory Barriers Are Hardware Specific</h3>
		<p>
			This article does not aim to be a comprehensive overview of all memory barriers.
			This would be a monumental task.
			But it is important though to appreciate the fact that these instructions vary considerably across different hardware architectures.
			Below is what the successive writes translate to on a multi-processing Intel Xeon E5410 and a SPARC workstation.
			All remaining assembly instruction sequences in this article were captured on an Intel Xeon unless specified otherwise.
		</p>
<table>
<tr>
<td>
A write followed by a write to shared memory on x86.
</td>
<td>
A write followed by a write to shared memory on SPARC.
</td>
</tr>
<tr>
<td valign="top">
<pre>
 1  0x03f8340c: push   %ebp               ;...55
 2  0x03f8340d: sub    $0x8,%esp          ;...81ec0800 0000
 3  0x03f83413: mov    $0x14c,%edi        ;...bf4c0100 00
 4  0x03f83418: movb   $0x1,-0x505a72f0(%edi)  ;...c687108d a5af01
 5  0x03f8341f: mfence                    ;...0faef0
 6  0x03f83422: mov    $0x148,%ebp        ;...bd480100 00
 7  0x03f83427: mov    $0x14d,%edx        ;...ba4d0100 00
 8  0x03f8342c: movsbl -0x505a72f0(%edx),%ebx  ;...0fbe9a10 8da5af
 9  0x03f83433: test   %ebx,%ebx          ;...85db
10  0x03f83435: jne    0x03f83460         ;...7529
11  0x03f83437: movl   $0x1,-0x505a72f0(%ebp)  ;...c785108d a5af01
12  0x03f83441: movb   $0x0,-0x505a72f0(%edi)  ;...c687108d a5af00
<b>13  0x03f83448: mfence                    ;...0faef0</b>
14  0x03f8344b: add    $0x8,%esp          ;...83c408
15  0x03f8344e: pop    %ebp               ;...5d
</pre>
</td>
<td>
<pre>
	<span style="color:red">TODO: get SPARC data</span>
</pre>
</td>
</tr>
</table>
		<p>
			Here we see both writes on lines 11 and 12 on the x86 Xeon.  
			The second write is chased with an mfence instruction, an explicit bidirectional memory barrier.
			<span style="color:red">TODO: discuss SPARC structions</span>.
			The JVM does not bother placing a memory barrier between these writes on either platform because it can trust x86 and SPARC hardware not to re-order writes.
			This is not true for Itanium, PowerPC or Alpha.
			Avoiding memory barriers this way is discussed in a later section.
		</p>
		<h3>Memory Barriers Can Be Explicit or Implicit</h3>
		<p>
			Explicit fence instructions are not the only way to serialize memory operations. 
			Let's switch gears to the Counter class to see an example.  
		</p>
		<pre>
    class Counter{
	
        static int counter = 0;
	
        public static void main(String[] _){
            for(int i = 0; i &lt; 100000; i++)
                inc();
        }
	
        static synchronized void inc(){ counter += 1; }
	
    }		
		</pre>
		<p>
			The Counter class performs a classic read-modify-write operation.  
			The static counter field is not volatile because all three operations must be observed atomically.  
			For this reason the inc method of the Counter class is synchronized.
			We can compile the Counter class and observe the generated assembly instructions for the synchronized inc method with the following command. 
			The Java memory model guarantees the same visibility semantics for exiting of synchronized regions as it does for
			volatile memory operations, so we should expect to see another memory barrier.
		</p>
		<div class="code">
		$ java -XX:+UnlockDiagnosticVMOptions -XX:PrintAssemblyOptions=hsdis-print-bytes -XX:-UseBiasedLocking -XX:CompileCommand=print,Counter.inc Counter
		</div>
		<pre>
   1: push   %ebp               ;...55
   2: mov    %esp,%ebp          ;...8bec
   3: sub    $0x28,%esp         ;...83ec28
   4: mov    $0x95ba5408,%esi   ;...be0854ba 95
   5: lea    0x10(%esp),%edi    ;...8d7c2410
   6: mov    %esi,0x4(%edi)     ;...897704
   7: mov    (%esi),%eax        ;...8b06
   8: or     $0x1,%eax          ;...83c801
   9: mov    %eax,(%edi)        ;...8907
  <span style="font-weight:bold">10: lock cmpxchg %edi,(%esi)  ;...f00fb13e</span>
  11: je     0x011f2dda         ;...0f841000 0000
  12: sub    %esp,%eax          ;...2bc4
  13: and    $0xfffff003,%eax   ;...81e003f0 ffff
  14: mov    %eax,(%edi)        ;...8907
  15: jne    0x011f2e11         ;...0f853700 0000
  16: mov    $0x95ba52b8,%eax   ;...b8b852ba 95
  17: mov    0x148(%eax),%esi   ;...8bb04801 0000
  <span style="font-weight:bold">18: inc    %esi               ;...46</span>
  19: mov    %esi,0x148(%eax)   ;...89b04801 0000
  20: lea    0x10(%esp),%eax    ;...8d442410
  21: mov    (%eax),%esi        ;...8b30
  22: test   %esi,%esi          ;...85f6
  23: je     0x011f2e07         ;...0f840d00 0000
  24: mov    0x4(%eax),%edi     ;...8b7804
  <span style="font-weight:bold">25: lock cmpxchg %esi,(%edi)  ;...f00fb137</span>
  26: jne    0x011f2e1f         ;...0f851800 0000
  27: mov    %ebp,%esp          ;...8be5
  28: pop    %ebp               ;...5d
		</pre>
		<p>
		  To no surprise the number of instructions generated by synchronized is more than volatile.  The increment is found on line 18 but 
		  at no point does the JVM insert an explicit memory barrier.  Instead, the JVM has killed two birds with one stone using a lock prefixed cmpxchg instruction 
		  on lines 10 and 25.  The semantics of cmpxchg are beyond the scope of this article.  
		  What's relevant is that 'lock cmpxchg' not only performs the write operation atomically - it also flushes pending read and write operations.  
		  The write operation will now become visible before all subsequent memory operations.
		  If we refactor and run the Counter class to use java.util.concurrent.atomic.AtomicInteger we can observe this same trick.
		</p>
<pre>
    import java.util.concurrent.atomic.AtomicInteger;
	
    class Counter{
	
        static AtomicInteger counter = new AtomicInteger(0);
	
        public static void main(String[] args){
            for(int i = 0; i &lt; 1000000; i++)
                counter.incrementAndGet(); 
        }
	
    }
</pre>		
		<div class="code">
		$ java -XX:+UnlockDiagnosticVMOptions -XX:PrintAssemblyOptions=hsdis-print-bytes -XX:CompileCommand=print,*AtomicInteger.incrementAndGet Counter
		</div>
<pre>
   1: push   %ebp               ;...55
   2: mov    %esp,%ebp          ;...8bec
   3: sub    $0x38,%esp         ;...83ec38
   4: jmp    0x043cb20a         ;...e9080000 00
   5: xchg   %ax,%ax            ;...6690
   6: test   %eax,0xb788d100    ;...850500d1 88b7
   7: mov    0x8(%ecx),%eax     ;...8b4108
   8: mov    %eax,%esi          ;...8bf0
   9: inc    %esi               ;...46
  10: mov    $0x9a3f03d0,%edi   ;...bfd0033f 9a
  11: mov    0x160(%edi),%edi   ;...8bbf6001 0000
  12: mov    %ecx,%edi          ;...8bf9
  13: add    $0x8,%edi          ;...83c708
  <span style="font-weight:bold">14: lock cmpxchg %esi,(%edi)  ;...f00fb137</span>
  15: mov    $0x1,%eax          ;...b8010000 00
  16: je     0x043cb234         ;...0f840500 0000
  17: mov    $0x0,%eax          ;...b8000000 00
  18: cmp    $0x0,%eax          ;...83f800
  19: je     0x043cb204         ;...74cb
  20: mov    %esi,%eax          ;...8bc6
  21: mov    %ebp,%esp          ;...8be5
  22: pop    %ebp               ;...5d
</pre>		
		<p>
			Again we see the write operation being combined with a lock prefix on line 14.  This insures the new 
			value of the variable will become visible to other threads before all subsequent memory operations.
		</p>		
		<h3>Memory Barriers Can Be Avoided</h3>
		<p>
			The JVM is very good at eliminating unnecessary memory barriers.
			Often it gets lucky and the consistency guarantees of the hardware memory model are greater than or equal to those of the Java memory model.  
			When this happens the JVM simply inserts a no op instead of an actual memory barrier.  
			For example, the consistency guarantees of the x86 and SPARC memory models are strong enough to eliminate the need for a memory barrier when reading a volatile variable.
			Remember the explicit unidirectional memory barrier used to separate both read operations on Itanium?
			Well, the generated assembly instructions for the successive reads in Dekker's algorithm on an x86 have <i>no</i> memory barrier.
		</p>
<table>
<tr>
<td>
A read followed by a read of shared memory on x86
</td>
</tr>
<tr>
<td valign="top"> 		
<pre>
 1  0x03f83422: mov    $0x148,%ebp        ;...bd480100 00
<b> 2  0x03f83427: mov    $0x14d,%edx        ;...ba4d0100 00</b>
 3  0x03f8342c: movsbl -0x505a72f0(%edx),%ebx  ;...0fbe9a10 8da5af
 4  0x03f83433: test   %ebx,%ebx          ;...85db
 5  0x03f83435: jne    0x03f83460         ;...7529
 6  0x03f83437: movl   $0x1,-0x505a72f0(%ebp)  ;...c785108d a5af01
 7  0x03f83441: movb   $0x0,-0x505a72f0(%edi)  ;...c687108d a5af00
 8  0x03f83448: mfence                    ;...0faef0
 9  0x03f8344b: add    $0x8,%esp          ;...83c408
10  0x03f8344e: pop    %ebp               ;...5d
11  0x03f8344f: test   %eax,0xb78ec000    ;...850500c0 8eb7
12  0x03f83455: ret                       ;...c3
13  0x03f83456: nopw   0x0(%eax,%eax,1)   ;...66660f1f 840000
<b>14  0x03f83460: mov    -0x505a72f0(%ebp),%ebx  ;...8b9d108d a5af</b>
15  0x03f83466: test   %edi,0xb78ec000    ;...853d00c0 8eb7
</pre>
</td>
</tr>
</table>	
		<p>
			The volatile read operations are found on lines two and twelve.
			Neither are not paired with a memory barrier.
			In other words the only performance penalty for a volatile read on an x86 (or on SPARC for that matter) is a minor loss of code motion optimization opportunities -
			the instruction itself is no different than an ordinary read.
		</p>
		<p>
			Unidirectional memory barriers are naturally less expensive than bidirectional ones.
			The JVM will avoid a bidirectional memory barrier when it knows a unidirectional one is sufficient.
			The first example in this article demonstrated this.  
			We saw that the successive read operations on Itanium were chased with an annotated with a unidirectional memory barrier.
			If the read operations had been annotated with an explicit bidirectional memory barrier the program would still be correct, but at a greater latency cost.
		</p>
		<p>
			On a uni-processor system the JVM can insert a no op for <i>all</i> memory barriers because memory operations are already serialized.  
			The following instruction stream was captured from a runtime compilation of two successive writes in Dekker's algorithm.
			The program was running in a VMWare workstation image in uni-processor mode on x86 hardware.
			Neither write operation is chased with a memory barrier.
  		</p>
<pre>
 1  0x03f8340c: push   %ebp               ;...55
 2  0x03f8340d: sub    $0x8,%esp          ;...81ec0800 0000
 3  0x03f83413: mov    $0x14c,%edi        ;...bf4c0100 00
 4  0x03f83418: movb   $0x1,-0x505a72f0(%edi)  ;...c687108d a5af01
 5  0x03f83422: mov    $0x148,%ebp        ;...bd480100 00
 6  0x03f83427: mov    $0x14d,%edx        ;...ba4d0100 00
 7  0x03f8342c: movsbl -0x505a72f0(%edx),%ebx  ;...0fbe9a10 8da5af
 8  0x03f83433: test   %ebx,%ebx          ;...85db
 9  0x03f83435: jne    0x03f83460         ;...7529
10  0x03f83437: movl   $0x1,-0x505a72f0(%ebp)  ;...c785108d a5af01
11  0x03f83441: movb   $0x0,-0x505a72f0(%edi)  ;...c687108d a5af00

<b>No Memory Barrier</b>

12  0x03f8344b: add    $0x8,%esp          ;...83c408
13  0x03f8344e: pop    %ebp               ;...5d
</pre>
		<h3>Conclusion</h3>
		<p>
			Modern hardware is not sequentially consistent.
			Memory barriers are consequently a necessity for a multi-threaded programs.
			They come in many flavors.
			Some are explicit, others are implicit.
			Some are bidirectional, others are unidirectional.
			The JVM uses this array of choices to efficiently honor the Java memory model across all platforms.
			I hope this article helps experienced JVM developers become a little more knowlegdeable about how their code behaves under the hood.
		</p>
		<h3>Acknowlegements</h3>
		<p>
			Thanks to Christian Thalinger of Sun Microsystems for access to a SPARC workstation. 
		</p>
		<h3>Reference Material</h3>
		<ul>
			<li><a href="http://www.intel.com/products/processor/manuals/" target="new">Intel 64 and IA-32 Architectures Software Developer's Manuals</a></li>
			<li><a href="http://www.csee.umbc.edu/help/architecture/aig.pdf" target="new">IA-64 Application Instruction Set Architecture Guide</a></li>
			<li><a href="http://www.amazon.com/Java-Concurrency-Practice-Brian-Goetz/dp/0321349601" target="new">Java Concurrency in Practice</a> by <a href="http://www.briangoetz.com/" target="new">Brian Goetz</a></li>
			<li><a href="http://g.oswego.edu/dl/jmm/cookbook.html" target="new">JSR-133 Cookbook</a> by Doug Lea</li>
			<li>Mutual exclusion with <a href="http://en.wikipedia.org/wiki/Dekker%27s_algorithm" target="new">Dekker's Algorithm</a></li>
			<li>Examining generated code with <a href="http://wikis.sun.com/display/HotSpotInternals/PrintAssembly" target="new">PrintAssembly</a></li>
			<li><a href="http://kenai.com/projects/base-hsdis/downloads" target="new">The Kenai Project</a> - a disassembler plugin</li>
			<li><a href="http://hg.openjdk.java.net/jdk7/hotspot/hotspot/file/tip/src/share/tools/hsdis/" target="new">hsdis</a> - a disassembler plugin</li>
		</ul>
		<h3>About the Author </h3>
		<p>
			<a href="http://notdennisbyrne.blogspot.com/">Dennis Byrne</a> is a senior software engineer for
			<a href="http://drwholdings.com/">DRW Trading</a>, a proprietary trading firm and liquidity provider.  
			He is a writer, presenter and active member of the open source community.
		</p>
	</body>
</html>